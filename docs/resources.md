# Things to Read, Review, and Learn

- [ ] GNNs
	- [ ] Attention
- [ ] LLMs
- [ ] Transformers
- [ ] Autoencoders
- [ ] Basics
	- [ ] Linear Algebra
	- [ ] Probability
	- [ ] Numerical Computation
	- [ ] Training


## Resources

## Classes

- [GNN Stanford](https://web.stanford.edu/class/cs224w/)
- [GNN Penn](https://gnn.seas.upenn.edu/)

## Topic Papers, Book Chapters, and Posts

* Graphical Neural Nets
	* [ ] [distill.pub](https://distill.pub/2021/gnn-intro/#graph-attention-networks)
- Attention
	- [ ] [Machine Learning Mastery](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)
- Generative Models
	- [ ] [Generative Adversarial Networks, Lillian Weng](https://lilianweng.github.io/posts/2017-08-20-gan/)
	- [ ] [Variational Auto Encoders, Lillian Weng](https://lilianweng.github.io/posts/2018-08-12-vae/)
	- [ ] [Deep Learning Book](https://www.deeplearningbook.org/contents/generative_models.html)
- Diffusion Models
	- [ ] [Lillian Weng](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/?utm_source=pocket_saves)
* Teacher Forcing
	* [ ] [Machine Learning Mastery](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)
* Attention and Augmented Recurrent Neural Networks
	* [ ] [distill.pub](https://distill.pub/2016/augmented-rnns/)
* Representation Learning
	* [ ] [Deep Learning Book](https://www.deeplearningbook.org/contents/representation.html)
* Transformers
	* [ ] [arxiv](https://arxiv.org/abs/2304.10557)
	* [ ] [Google Research](https://blog.research.google/2017/08/transformer-novel-neural-network.html)
* Protein Language Models
	* [ ] [Rives 2021, PNAS](https://www.pnas.org/doi/epdf/10.1073/pnas.2016239118)
	* [ ] [Lin 2023, Science](https://www.science.org/doi/10.1126/science.ade2574)
* Autoencoders
	* [ ] [Deep Learning Book](https://www.deeplearningbook.org/contents/autoencoders.html)
* Basics
	* [ ] [Deep Learning Book](https://www.deeplearningbook.org/)
